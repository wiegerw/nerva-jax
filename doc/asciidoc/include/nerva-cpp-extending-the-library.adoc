[[extending]]
== Extending the library
The {library} can be extended in several obvious ways, such as adding new layers, activation functions, loss functions, learning rate schedulers and pruning or growing functions. This can be done by inheriting from the appropriate base class and implementing the required virtual functions. The table below provides an overview:

|===
|Functionality |Base class

|A layer
|`neural_network_layer`

|An activation function
|`activation_function`

|A loss function
|`loss_function`

|A learning rate scheduler
|`learning_rate_scheduler`

|A pruning function
|`prune_function`

|A growing function
|`grow_function`
|===

It is recommended to follow the approach advocated in the Nerva libraries. Each implementation should be based on a mathematical specification, as explained in the paper https://arxiv.org/abs/2511.11918[Batch Matrix-form Equations and Implementation of Multilayer Perceptrons].
After defining the mathematical equations, you can use the table of <<table_matrix_operations>> to convert the equations into code.

Another crucial step is validation and testing. The symbolic mathematics library
https://docs.sympy.org/latest/index.html[SymPy]
can be used to validate the equations.
The https://github.com/wiegerw/nerva-sympy[nerva-sympy] repository contains https://github.com/wiegerw/nerva-sympy/tree/main/tests[test cases] for activation functions, loss functions, layers, and even for the derivation of equations.
