== Sparse neural networks
Sparse neural network layers are often simulated using binary masks, see cite:[DBLP_journals_corr_abs-2102-01732]. This is caused by the lack of support for sparse tensors in popular neural network frameworks. Note that PyTorch is currently developing https://pytorch.org/docs/stable/sparse.html[sparse tensors]. The {library} supports truly sparse layers, meaning that the weight matrices of sparse layers are stored in a sparse matrix format. Another example of truly sparse layers is given by cite:[DBLP_conf_icml_NikdanPIKA23].

=== Sparse matrices
Since we are dealing with a programming context, we say that the _support_ of a sparse matrix refers to the set of positions (or indices) in the matrix that are explicitly stored. Elements inside the support can have a non-zero value. Elements outside the support have the value zero by definition.

Sparse matrices in the {library} are stored in
link:++https://en.wikipedia.org/wiki/Sparse_matrix#Compressed_sparse_row_(CSR,_CRS_or_Yale_format)++[CSR] format.
This matrix representation stores arrays of column and row indices to define the support, plus an array of the corresponding values. CSR matrices are _unstructured_ sparse matrices, meaning they have non-zero elements located at arbitrary positions. Alternatively, there are _structured_ sparse matrices, take for example butterfly matrices cite:[DBLP_journals_corr_abs-2405-15013].

=== Sparse evolutionary training
Sparse evolutionary training (SET) is a method for efficiently training sparse neural networks, see e.g. cite:[DBLP_journals_nca_LiuMMPP21]. The idea behind this method is to start the training with a random sparse topology, and to periodically prune and regrow some of the weights.

=== Sparse initialization
In SET, the sparsity is not divided evenly over the sparse layers. Instead, small layers are assigned a higher density than larger ones.
In cite:[DBLP_journals_nca_LiuMMPP21] formula (3), https://en.wikipedia.org/wiki/Erd%C5%91s%E2%80%93R%C3%A9nyi_model[Erdős–Rényi graph topology]
is suggested to calculate the densities of the sparse layers given a desired overall density of the sparse layers combined.
In the {library} this is implemented in the function `compute_sparse_layer_densities`, see `link:https://github.com/wiegerw/nerva-rowwise/blob/main/include/nerva/neural_networks/layer_algorithms.h[layer_algorithms.h]`. The original Python implementation can be found
https://github.com/VITA-Group/Random_Pruning/blob/871077f1d10f9bc44941b093fd5ccbc4ec3984fa/CIFAR/sparselearning/core.py#L155[here], along with several other sparse initialization strategies.
In the tool `{mlptool}` the option `--overall-density` is used for assigning Erdős–Rényi densities to the sparse layers. See <<mlp_output>> for an example of this. The overall density of `0.05` is converted into densities `[0.042382877, 0.06357384, 1.0]` for the individual layers.

=== Pruning weights
Pruning weights is about removing parameters from a neural network, see also https://en.wikipedia.org/wiki/Pruning_(artificial_neural_network)[Pruning (artificial_neural_network)]. In our context removing parameters is about removing elements from the support of a sparse weight matrix. The effect of this is that the values corresponding to these elements are zeroed.

==== Threshold pruning
In threshold pruning, all weights stem:[w_{ij}] with stem:[|w_{ij}| \leq t] for a given threshold stem:[t] are pruned from a weight matrix stem:[W].

==== Magnitude based pruning
Magnitude based pruning is special case of threshold pruning. In magnitude based pruning, the threshold stem:[t] is computed such that for a given fraction stem:[\zeta] of the weights we have stem:[|w_{ij}| \leq t]. To ensure that the desired fraction of weights is removed, our implementation takes into account that there can be multiple weights with
stem:[|w_{ij}| = t].

==== SET based pruning
In SET based pruning, magnitude pruning is applied to positive weights and negative weights separately. So a fraction stem:[\zeta] of the positive weights and a fraction stem:[\zeta] of the negative weights are pruned.

=== Growing weights
Growing weights is about adding parameters to a neural network. In our context adding parameters is about adding elements to the support of a sparse weight matrix.

==== Random growing
In random growing, a given number of elements is chosen randomly from the positions outside the support of a weight matrix. These new elements are then added to the support. Since the new elements need to be initialized, a weight initializer needs to be chosen to generate values for them.

A specific implementation of random growing for matrices in CSR format has been developed, that uses https://en.wikipedia.org/wiki/Reservoir_sampling[reservoir sampling] to determine the new elements that are added to the support.
