== Matrix operations
The most important part of the implementation of neural networks consists of matrix operations. In the implementation of activation functions, loss functions and neural network layers, many different matrix operations are needed. In Nerva a structured approach is followed to implement these components. All equations are expressed in terms of the matrix operations in the table below.

.matrix operations
[[table_matrix_operations]]
|===
|Operation |Code |Definition

|stem:[0_{m}]
|`zeros(m)`
|stem:[m \times 1] column vector with elements equal to 0

|stem:[0_{mn}]
|`zeros(m, n)`
|stem:[m \times n] matrix with elements equal to 0

|stem:[1_{m}]
|`ones(m)`
|stem:[m \times 1] column vector with elements equal to 1

|stem:[1_{mn}]
|`ones(m, n)`
|stem:[m \times n] matrix with elements equal to 1

|stem:[\mathbb{I}_n]
|`identity(n)`
|stem:[n \times n] identity matrix

|stem:[X^\top]
|`X.transpose()`
|transposition

|stem:[cX]
|`c * X`
|scalar multiplication, stem:[c \in \mathbb{R}]

|stem:[X + Y]
|`X + Y`
|addition

|stem:[X - Y]
|`X - Y`
|subtraction

|stem:[X \cdot Z]
|`X * Z`
|matrix multiplication, also denoted as stem:[XZ]

|stem:[x^\top y~] or stem:[~x y^\top]
|`dot(x,y)`
|dot product, stem:[x,y \in \mathbb{R}^{m \times 1}] or stem:[x,y \in \mathbb{R}^{1 \times n}]

|stem:[X \odot Y]
|`hadamard(X,Y)`
|element-wise product of stem:[X] and stem:[Y]

|stem:[\mathsf{diag}(X)]
|`diag(X)`
|column vector that contains the diagonal of stem:[X]

|stem:[\mathsf{Diag}(x)]
|`Diag(x)`
|diagonal matrix with stem:[x] as diagonal, stem:[x \in \mathbb{R}^{1 \times n}]  or stem:[x \in \mathbb{R}^{m \times 1}]

|stem:[1_m^\top \cdot X \cdot 1_n]
|`elements_sum(X)`
|sum of the elements of stem:[X]

|stem:[x \cdot 1_n^\top]
|`column_repeat(x, n)`
|stem:[n] copies of column vector stem:[x \in \mathbb{R}^{m \times 1}]

|stem:[1_m \cdot x]
|`row_repeat(x, m)`
|stem:[m] copies of row vector stem:[x \in \mathbb{R}^{1 \times n}]

|stem:[1_m^\top \cdot X]
|`columns_sum(X)`
|stem:[1 \times n] row vector with sums of the columns of stem:[X]

|stem:[X \cdot 1_n]
|`rows_sum(X)`
|stem:[m \times 1] column vector with sums of the rows of stem:[X]

|stem:[\max(X)_{col}]
|`columns_max(X)`
|stem:[1 \times n] row vector with maximum values of the columns of stem:[X]

|stem:[\max(X)_{row}]
|`rows_max(X)`
|stem:[m \times 1] column vector with maximum values of the rows of stem:[X]

|stem:[(1_m^\top \cdot X) / n]
|`columns_mean(X)`
|stem:[1 \times n] row vector with mean values of the columns of stem:[X]

|stem:[(X \cdot 1_n) / m]
|`rows_mean(X)`
|stem:[m \times 1] column vector with mean values of the rows of stem:[X]

|stem:[f(X)]
|`apply(f, X)`
|element-wise application of stem:[f: \mathbb{R} \rightarrow \mathbb{R}] to stem:[X]

|stem:[e^X]
|`exp(X)`
|element-wise application of stem:[f: x \rightarrow e^x] to stem:[X]

|stem:[\log(X)]
|`log(X)`
|element-wise application of the natural logarithm stem:[f: x \rightarrow \ln(x)] to stem:[X]

|stem:[1 / X]
|`reciprocal(X)`
|element-wise application of stem:[f: x \rightarrow 1/x] to stem:[X]

|stem:[\sqrt{X}]
|`sqrt(X)`
|element-wise application of stem:[f: x \rightarrow \sqrt{x}] to stem:[X]

|stem:[X^{-1/2}]
|`inv_sqrt(X)`
|element-wise application of stem:[f: x \rightarrow x^{-1/2}] to stem:[X]

|stem:[\log(\sigma(X))]
|`log_sigmoid(X)`
|element-wise application of stem:[f: x \rightarrow \log(\sigma(x))] to stem:[X]
|===

Using this table leads to concise and uniform code. For example, the backpropagation implementation of a softmax layer looks like this:
[.small-code]
[source,cpp]
----
include::../../include/nerva/neural_networks/layers.h[tag=matrix_operations]
----
See the specification document https://wiegerw.github.io/nerva-rowwise/pdf/nerva-library-specifications.pdf[Nerva library specifications] for an overview of how these matrix operations are used.

=== Eigen library
The {library} uses the https://eigen.tuxfamily.org/[Eigen library] for representing matrices. The matrix operations in table <<table_matrix_operations>> have been implemented using Eigen, see the file https://github.com/wiegerw/nerva-rowwise/blob/main/include/nerva/neural_networks/matrix_operations.h[matrix_operations.h].

=== MKL library
Using the Eigen library alone is not sufficient for obtaining high performance. Therefore, the {library} uses the https://en.wikipedia.org/wiki/Math_Kernel_Library[Intel Math Kernel library (MKL)] as a backend. The Eigen library supports MKL by means of the compiler flag `EIGEN_USE_MKL_ALL`, see also https://eigen.tuxfamily.org/dox/TopicUsingIntelMKL.html[TopicUsingIntelMKL.html].
Note that the MKL library is included in the https://www.intel.com/content/www/us/en/developer/tools/oneapi/base-toolkit-download.html[Intel oneAPI base toolkit].

The MKL library supports a number of highly efficient, but extremely low-level interfaces for matrix operations. See https://www.intel.com/content/www/us/en/docs/onemkl/developer-reference-c/2024-2/blas-and-sparse-blas-routines.html[blas-and-sparse-blas-routines.html] for an overview. The {library} contains matrix classes that hide those low-level details from the user. The table below gives an overview of them.

|===
|Header file |Description

|`link:https://github.com/wiegerw/nerva-rowwise/blob/main/include/nerva/neural_networks/mkl_dense_vector.h[mkl_dense_vector.h]`
|A class `dense_vector_view` that wraps a raw pointer to a vector.

|`link:https://github.com/wiegerw/nerva-rowwise/blob/main/include/nerva/neural_networks/mkl_dense_matrix.h[mkl_dense_matrix.h]`
|A class `dense_matrix_view` that wraps a raw pointer to a matrix, and a class
`dense_matrix` that stores a dense matrix.

|`link:https://github.com/wiegerw/nerva-rowwise/blob/main/include/nerva/neural_networks/mkl_sparse_matrix.h[mkl_sparse_matrix.h]`
|A class `sparse_matrix_csr` footnote:[Note that the name `sparse_matrix` could not be used due to a conflict with a #define of the same name buried deep inside the MKL library code] that stores a sparse matrix in
https://en.wikipedia.org/wiki/Sparse_matrix#Compressed_sparse_row_(CSR,_CRS_or_Yale_format)[compressed sparse row] (CSR) format.
|===

TIP: In C++23 the implementation of sparse matrices in CSR format can be greatly simplified, as shown by https://github.com/BenBrock/matrix-cpos[Ben Brock].

NOTE: The sparse CSR matrix functions in the MKL library take an argument of the opaque type `sparse_matrix_t`. It stores unspecified properties of a sparse matrix. This parameter is poorly documented, and it is unknown when this parameter should be recalculated. For safety reasons, the {library} recalculates this parameter after every change to a sparse matrix, which may cause some inefficiencies. See also the function `sparse_matrix_csr::construct_csr` and
https://www.intel.com/content/www/us/en/docs/onemkl/developer-reference-c/2024-1/mkl-sparse-create-csr.html[mkl_sparse_?_create_csr].
