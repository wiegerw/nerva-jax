:root: ../../

== Performance
This section discusses various aspects that play a role for the performance of a neural network library.

=== Mini-batches
In textbooks and tutorials, the training of a neural network is usually explained in terms of individual examples. But in order to achieve high performance, it is absolutely necessary to use mini-batches. On https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Iterative_method[Wikipedia] this is explained as follows:

____
A compromise between computing the true gradient and the gradient at a single sample is to compute the gradient against more than one training sample (called a "mini-batch") at each step. This can perform significantly better than "true" stochastic gradient descent described, because the code can make use of vectorization libraries rather than computing each step separately.
____

To support mini-batches, the {library} defines all equations that play a role in the execution of a neural network in matrix form, including the backpropagation equations, see the specification document https://wiegerw.github.io/nerva-rowwise/pdf/nerva-library-specifications.pdf[Nerva library specifications]. For the latter, many neural network frameworks rely on https://en.wikipedia.org/wiki/Automatic_differentiation[Automatic differentiation], see also cite:[DBLP_journals_jmlr_BaydinPRS17]. We use explicit backpropagation equations to implement truly sparse layers and to provide an instructive resource for those studying neural network execution.

=== Matrix products
The performance of training a neural network largely depends on the calculation of matrix products during the backpropagation step of linear layers. In order to do this efficiently, the https://en.wikipedia.org/wiki/Math_Kernel_Library[Intel Math Kernel library (MKL)] is used. Currently, this dependency is hard coded, but there are plans to make this optional. To experiment with other implementations, like SYCL or BLAS, a global setting is used that is discussed in the next section.

=== Nerva computation mode
In general, the performance of Eigen is very good. But occasionally, the generated code for a matrix expression can be quite poor. Especially in case of backpropagation calculations this can have a huge impact on the performance. The {library} uses a global setting `NervaComputation` to experiment with other implementations. For example, the function `softmax_layer::feedforward` contains this:
[.small-code]
[source,cpp]
----
include::{root}/include/nerva/neural_networks/layers.h[tag=nerva_computation]
----
In this case, the default version `computation::eigen` turned out to have very poor performance. A direct call to an MKL routine is used to solve this problem. The `NervaComputation` setting is also used to experiment with BLAS implementations and SYCL implementations. See the file https://github.com/wiegerw/nerva-rowwise/blob/main/include/nerva/neural_networks/optimizers.h[optimizers.h] for some examples of that.

TIP: The command line tool `mlp` has an option `--computation` to set the computation mode.

=== Subnormal numbers
Experiments with sparse neural networks have shown that the performance can be negatively influenced by https://en.wikipedia.org/wiki/Subnormal_number[subnormal numbers]. The example program `link:https://github.com/wiegerw/nerva-rowwise/blob/main/examples/subnormal_numbers.cpp[subnormal_numbers.cpp]` illustrates the problem. The table below is the result of the following experiment.
The dot product of two large vectors of floating-point numbers is computed. One vector is filled with random values between 0 and 1, and the other with powers of 10, ranging from `1` to `1e−45`. For values larger than `1e−35`, the time needed for this calculation is about `0.044` seconds. For smaller values we end up in the range of subnormal numbers. This causes the runtime to increase more than eight-fold to `0.37` seconds. In our experiments we observed that when layers with high sparsity are used, it may happen that subnormal values appear in weight matrices, and their amount increases every epoch.
[.small-code]
[listing]
----
--- multiplication1 ---
time =   0.044372 | value = 1.0e+00    | sum = -5.49552e+03
time =   0.044567 | value = 1.0e-01    | sum = -5.49572e+02
time =   0.044243 | value = 1.0e-02    | sum = -5.49304e+01
time =   0.044434 | value = 1.0e-03    | sum = -5.49612e+00
time =   0.044253 | value = 1.0e-04    | sum = -5.49862e-01
time =   0.044765 | value = 1.0e-05    | sum = -5.49653e-02
time =   0.044698 | value = 1.0e-06    | sum = -5.49624e-03
time =   0.044683 | value = 1.0e-07    | sum = -5.49642e-04
time =   0.044703 | value = 1.0e-08    | sum = -5.49491e-05
time =   0.044821 | value = 1.0e-09    | sum = -5.49454e-06
time =   0.044705 | value = 1.0e-10    | sum = -5.49557e-07
time =   0.044657 | value = 1.0e-11    | sum = -5.49730e-08
time =   0.045235 | value = 1.0e-12    | sum = -5.49563e-09
time =   0.045120 | value = 1.0e-13    | sum = -5.49706e-10
time =   0.045010 | value = 1.0e-14    | sum = -5.49719e-11
time =   0.044988 | value = 1.0e-15    | sum = -5.49464e-12
time =   0.044943 | value = 1.0e-16    | sum = -5.49629e-13
time =   0.044795 | value = 1.0e-17    | sum = -5.49573e-14
time =   0.044147 | value = 1.0e-18    | sum = -5.49449e-15
time =   0.044166 | value = 1.0e-19    | sum = -5.49589e-16
time =   0.044380 | value = 1.0e-20    | sum = -5.49722e-17
time =   0.044036 | value = 1.0e-21    | sum = -5.49430e-18
time =   0.043405 | value = 1.0e-22    | sum = -5.49577e-19
time =   0.043615 | value = 1.0e-23    | sum = -5.49548e-20
time =   0.043544 | value = 1.0e-24    | sum = -5.49570e-21
time =   0.043547 | value = 1.0e-25    | sum = -5.49694e-22
time =   0.043536 | value = 1.0e-26    | sum = -5.49365e-23
time =   0.043560 | value = 1.0e-27    | sum = -5.49488e-24
time =   0.043500 | value = 1.0e-28    | sum = -5.49657e-25
time =   0.043524 | value = 1.0e-29    | sum = -5.49783e-26
time =   0.044128 | value = 1.0e-30    | sum = -5.49559e-27
time =   0.043585 | value = 1.0e-31    | sum = -5.49745e-28
time =   0.043530 | value = 1.0e-32    | sum = -5.49488e-29
time =   0.043609 | value = 1.0e-33    | sum = -5.49569e-30
time =   0.043805 | value = 1.0e-34    | sum = -5.49446e-31
time =   0.046169 | value = 1.0e-35    | sum = -5.49661e-32
time =   0.070594 | value = 1.0e-36    | sum = -5.49664e-33
time =   0.247938 | value = 1.0e-37    | sum = -5.49684e-34
time =   0.368848 | value = 1.0e-38    | sum = -5.49553e-35
time =   0.369819 | value = 1.0e-39    | sum = -5.49426e-36
time =   0.368434 | value = 1.0e-40    | sum = -5.49607e-37
time =   0.368747 | value = 1.0e-41    | sum = -5.49801e-38
time =   0.369033 | value = 1.0e-42    | sum = -5.50173e-39
time =   0.370241 | value = 9.9e-44    | sum = -5.47762e-40
time =   0.370065 | value = 9.8e-45    | sum = -4.97559e-41
time =   0.370310 | value = 1.4e-45    | sum = -1.44152e-41
----
On https://groups.google.com/g/llvm-dev/c/TDGKHFU4hzE/m/k-LEa3NvBQAJ[Google Groups] this problem is discussed. A possible solution is to instruct the compiler to flush subnormal values to zero. But there doesn't seem to be a portable way to achieve this. In the {library} different solutions have been tried. One of them is to periodically flush weights in the subnormal range to zero using the `--clip` command line option of the `mlp` tool.
In cite:[DBLP_journals_actanum_HighamM22] the problem of subnormal numbers is discussed.
