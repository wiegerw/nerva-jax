== Command line tools
The following command line tools are available. They can be found in the `tools` directory.

|===
|Tool |Description

|`mlp`
|A tool for training multilayer perceptrons.

|`mkl`
|A tool for benchmarking sparse and dense matrix products using the Intel MKL library.

|`inspect_npz`
|A tool for inspecting the contents of a file in NumPy NPZ format.
|===

=== The tool mlp
The tool `mlp` can be used for training multilayer perceptrons. An example invocation of the `mlp` tool is

[.small-code]
[source,bash]
----
include::../../examples/cifar10_sparse.sh[tag=doc]
----
This will train a https://www.cs.toronto.edu/~kriz/cifar.html[CIFAR-10] model using an MLP consisting of three sparse layers with activation functions ReLU, ReLU and no activation.
A script `prepare_data.py` is available in the `data` directory
that can be used to download the dataset, flatten it and store it in `.npz` format. See the section <<preparing-data>> for details.

The output may look like this:
[[mlp_output]]
[.small-code]
[listing]
----
=== Nerva c++ model ===
Sparse(input_size=3072, output_size=1024, density=0.042382877, optimizer=Nesterov(0.90000), activation=ReLU())
Sparse(input_size=1024, output_size=1024, density=0.06357384, optimizer=Nesterov(0.90000), activation=ReLU())
Dense(input_size=1024, output_size=10, optimizer=Nesterov(0.90000), activation=NoActivation())
loss = SoftmaxCrossEntropyLoss()
scheduler = ConstantScheduler(lr=0.01)
layer densities: 133325/3145728 (4.238%), 66662/1048576 (6.357%), 10240/10240 (100%)

epoch   0 lr: 0.01000000  loss: 2.30284437  train accuracy: 0.07904000  test accuracy: 0.08060000 time: 0.00000000s
epoch   1 lr: 0.01000000  loss: 2.14723837  train accuracy: 0.21136000  test accuracy: 0.21320000 time: 2.74594253s
epoch   2 lr: 0.01000000  loss: 1.91454245  train accuracy: 0.29976000  test accuracy: 0.29940000 time: 2.76982510s
epoch   3 lr: 0.01000000  loss: 1.78019225  train accuracy: 0.35416000  test accuracy: 0.35820000 time: 2.69554319s
epoch   4 lr: 0.01000000  loss: 1.68071066  train accuracy: 0.39838000  test accuracy: 0.40000000 time: 2.68532307s
epoch   5 lr: 0.01000000  loss: 1.59761505  train accuracy: 0.42820000  test accuracy: 0.43060000 time: 3.02131606s
----

include::command-line-tools.adoc[tags=mlptool-options]

include::command-line-tools.adoc[tags=general-options]

* `--verbose`, `-v`
Show verbose output.

include::command-line-tools.adoc[tags=random-generator-options]

include::command-line-tools.adoc[tags=layer-configuration-options]

include::command-line-tools.adoc[tags=training-configuration-options]

include::command-line-tools.adoc[tags=dataset-options]

* `--generate-data <name>`
Specify a synthetic dataset that is generated on the fly. The following datasets are supported:
|===
|Specification |Description |Features |Classes

|`checkerboard`
|A checkerboard pattern, see also link:https://kaifishr.github.io/2021/01/14/micro-mlp.html#checkerboard[checkerboard].
|2
|2

|`mini`
|A dataset with random values.
|3
|2
|===

* `--dataset-size <value>`
The size of the generated dataset (default: 1000).
`--save-weights` for information about the format.
`--load-weights` for information about the format.

include::command-line-tools.adoc[tags=pruning-options]

include::command-line-tools.adoc[tags=computation-options]

include::command-line-tools.adoc[tags=miscellaneous-options]

=== The tool mkl
The tool `mkl` is used for benchmarking sparse and dense matrix products. An example of running the `mkl` tool is
[.small-code]
[source,bash]
----
include::../../examples/mkl_benchmark.sh[tag=doc]
----
This will use various algorithms to calculate the product `A = B * C` with `A` a sparse matrix and `B` and `C` dense matrices.

The output may look like this
[.small-code]
[listing]
----
--- testing A = B * C (sdd_product) ---
A = 1000x1000 sparse
B = 1000x1000 dense  layout=column-major
C = 1000x1000 dense  layout=column-major

density(A) = 0.5
 0.01147s ddd_product A=column-major, B=column-major, C=column-major
 0.00793s ddd_product A=column-major, B=column-major, C=column-major
 0.00854s ddd_product A=column-major, B=column-major, C=column-major
 0.04049s sdd_product(batchsize=5, density(A)=0.499599, B=column-major, C=column-major)
 0.01998s sdd_product(batchsize=5, density(A)=0.499599, B=column-major, C=column-major)
 0.01178s sdd_product(batchsize=5, density(A)=0.499599, B=column-major, C=column-major)
 0.01114s sdd_product(batchsize=10, density(A)=0.499599, B=column-major, C=column-major)
 0.01099s sdd_product(batchsize=10, density(A)=0.499599, B=column-major, C=column-major)
 0.00666s sdd_product(batchsize=10, density(A)=0.499599, B=column-major, C=column-major)
 0.00375s sdd_product(batchsize=100, density(A)=0.499599, B=column-major, C=column-major)
 0.00734s sdd_product(batchsize=100, density(A)=0.499599, B=column-major, C=column-major)
 0.00332s sdd_product(batchsize=100, density(A)=0.499599, B=column-major, C=column-major)
 0.20097s sdd_product_forloop_eigen(density(A)=0.499599, B=column-major, C=column-major)
 0.19891s sdd_product_forloop_eigen(density(A)=0.499599, B=column-major, C=column-major)
 0.19893s sdd_product_forloop_eigen(density(A)=0.499599, B=column-major, C=column-major)
 0.23286s sdd_product_forloop_mkl(density(A)=0.499599, B=column-major, C=column-major)
 0.23298s sdd_product_forloop_mkl(density(A)=0.499599, B=column-major, C=column-major)
 0.23281s sdd_product_forloop_mkl(density(A)=0.499599, B=column-major, C=column-major)
----
Note that the very first invocation of an MKL function can be slow.

=== The tool inspect_npz
The tool `inspect_npz` is a simple tool to show the contents of a file in NumPy NPZ format. The tool `mlp` uses this format to load and save datasets, and to load and save weight matrices + bias vectors of linear layers. The output may look like this:
[.small-code]
[listing]
----
W1 (1024x3072) norm = 0.03827324
   [-0.00850412,  0.00766624, -0.00379110,  ..., -0.02755435,  0.00842837,  0.00725122]
   [ 0.03012662, -0.01122476,  0.03765349,  ...,  0.02167689, -0.03734717, -0.01376905]
   [-0.03415587, -0.00498827,  0.00635345,  ..., -0.03036389, -0.01967963,  0.03339641]
   ...,
   [ 0.02993325, -0.00795984,  0.00388659,  ...,  0.01343446, -0.01625269,  0.00398590]
   [ 0.03800971, -0.01185982, -0.00944855,  ...,  0.02083720, -0.00217844,  0.02398606]
   [-0.00879488, -0.01937520, -0.02830209,  ...,  0.03606736, -0.01065827,  0.03293588]
b1= (1024)
   [-0.01735129, -0.01381215,  0.01708755,  ..., -0.01117092, -0.00264273, -0.00976263]
W2 (512x1024) norm = 0.06249978
   [-0.02440289,  0.01362467,  0.03782336,  ...,  0.01342138, -0.01060697, -0.05055390]
   [ 0.06187645, -0.00854158,  0.02849235,  ...,  0.05861567,  0.00708143, -0.06170959]
   [-0.00756755,  0.04718670, -0.02303848,  ...,  0.01513476,  0.00205931,  0.05441900]
   ...,
   [-0.04223771,  0.00852190, -0.00465803,  ...,  0.03600422,  0.00484904, -0.02281546]
   [ 0.03211500, -0.02740303, -0.04652309,  ...,  0.00307061,  0.02427530, -0.02245107]
   [ 0.05210501, -0.00423148, -0.00633851,  ...,  0.02453317,  0.02723335,  0.03589169]
b2= (512)
   [-0.01871627,  0.01150464, -0.01767523,  ..., -0.00220927, -0.01791467, -0.02616516]
W3 (10x512) norm = 0.10718583
   [-0.03256247, -0.09669271, -0.06564181,  ...,  0.00394586, -0.02191557,  0.08828022]
   [-0.09986399, -0.03712691,  0.04332626,  ..., -0.02475236, -0.07359495, -0.09421349]
   [-0.03308030,  0.01280271,  0.09341474,  ..., -0.03470980, -0.03936023,  0.02204999]
   ...,
   [-0.10063093, -0.04294113, -0.04938528,  ...,  0.08151620, -0.00991420,  0.09686699]
   [ 0.04347997, -0.08046009,  0.02828473,  ...,  0.06899156, -0.08314995,  0.07181197]
   [ 0.00575207, -0.06347645, -0.07257712,  ..., -0.00293436, -0.00266003, -0.08468610]
b3= (10)
   [-0.02117447, -0.00115431, -0.03672279,  ..., -0.02902718, -0.02759255,  0.03007624]
----
